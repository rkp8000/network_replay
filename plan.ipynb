{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. When does changing neural excitability maximally influence network dynamics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "$\\Delta \\mathbf{x}^{(i)} \\equiv (0, ..., 0, \\Delta x, 0, ..., 0) \\textrm{ where the only nonzero element is at position } i \\textrm{ (neuron } i \\textrm{)}, \\textrm{ (i.e. excitability change vector)}$\n",
    "\n",
    "$\\mathbf{x} \\equiv \\textrm{baseline excitability vector}$\n",
    "\n",
    "$W \\equiv \\textrm{synaptic connectivity matrix}$\n",
    "\n",
    "$\\mathbf{r}(t) \\equiv \\textrm{ntwk dynamics (multivariate time series)}$\n",
    "\n",
    "$Q[P(\\mathbf{r}(t)|\\Delta \\mathbf{x} = \\mathbf{0}; W, \\mathbf{x}), P(\\mathbf{r}(t)|\\Delta \\mathbf{x} = \\Delta \\mathbf{x}^{(i)}; W, \\mathbf{x})] \\equiv \\textrm{distance metric between ntwk dynamics distributions with and without excitability change}$\n",
    "\n",
    "In particular, let\n",
    "\n",
    "$Q_{i} = \\lVert \\mathop{\\mathbb{E}}[\\mathbf{r}(t)|\\Delta \\mathbf{x} = \\mathbf{0}; W, \\mathbf{x}] - \\mathop{\\mathbb{E}}[\\mathbf{r}(t)|\\Delta \\mathbf{x} = \\Delta \\mathbf{x}^{(i)}; W, \\mathbf{x}] \\rVert$\n",
    "\n",
    "where the expectation is both a time and ensemble average. Thus, $Q_{i}$ measures the overlap between the average set of neurons active in the baseline distribution vs that conditioned by $\\Delta \\mathbf{x}^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given $\\{W, \\mathbf{x}\\}$, which $\\Delta \\mathbf{x}^{(i)}$ maximize $Q_{i}$?\n",
    "\n",
    "### Computation\n",
    "\n",
    "This is likely an intractable optimization problem.\n",
    "\n",
    "However, we can make inroads by comparing standard connectivity structures.\n",
    "\n",
    "In particular, for a given connectivity structure (e.g. Erdos-Renyi, scale free, small world), we can estimate $Q_i$ for each neuron for simulations and plot the result as a function of the neuron's local connectivity patterns (e.g. in- & out-degree, centrality, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which $W$ maximizes $\\mathop{\\mathbb{E}_i}[Q_i]$?\n",
    "\n",
    "Again, this is probably intractable.\n",
    "\n",
    "However, we can compute distributions over $Q_i$ for different example $W$, allowing us to make statements about what global connectivity features enable excitability changes to have the most influence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. What network structures optimize excitability-controlled sequence generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions:\n",
    "\n",
    "$\\Delta \\mathbf{x} \\equiv \\textrm{excitability change vector}$\n",
    "\n",
    "$\\mathbf{x} \\equiv \\textrm{baseline excitability vector}$\n",
    "\n",
    "$W \\equiv \\textrm{synaptic connectivity matrix}$\n",
    "\n",
    "$\\mathbf{r}(t) \\equiv \\textrm{ntwk dynamics multivariate time series (\"song\")}$\n",
    "\n",
    "$S(\\mathbf{r}_i(t), \\mathbf{r}_j(t)) \\equiv \\textrm{sequence-based similarity metric between ntwk songs}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, let:\n",
    "\n",
    "$D_x^y = \\cfrac{\\mathop{{}\\mathbb{E}}[S_{in}]}{\\mathop{{}\\mathbb{E}}[S_{bt}]}$\n",
    "\n",
    "i.e. the ratio of sequence similarity given the same vs. different $\\Delta \\mathbf{x}$.\n",
    "\n",
    "This increases when (1) a given $\\Delta \\mathbf{x}$ reliably generates the same activity sequence, and (2) different $\\Delta \\mathbf{x}$'s generate different activity sequences. Thus, $D_x^y$ quantifies the extent to which different $\\Delta \\mathbf{x}$ yield consistent and distinguishable network dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete case:\n",
    "\n",
    "$\\Delta \\mathbf{x} \\in \\{\\Delta \\mathbf{x}_1, \\Delta \\mathbf{x}_2, ...\\}$\n",
    "\n",
    "$\n",
    "\\mathop{{}\\mathbb{E}}[S_{in}] \\equiv\n",
    "\\sum_k P(\\Delta \\mathbf{x}_k)\n",
    "\\sum_{i,j} \n",
    "P(\\mathbf{y}_i|\\Delta \\mathbf{x}_k; W, \\mathbf{x})\n",
    "P(\\mathbf{y}_j|\\Delta \\mathbf{x}_k; W, \\mathbf{x})\n",
    "S(\\mathbf{y}_i, \\mathbf{y}_j)\n",
    "$\n",
    "\n",
    "$\n",
    "\\mathop{{}\\mathbb{E}}[S_{bt}] \\equiv \n",
    "\\sum_{k,l} P(\\Delta \\mathbf{x}_k)P(\\Delta \\mathbf{x}_l)\n",
    "\\sum_{i,j}\n",
    "P(\\mathbf{y}_i|\\Delta \\mathbf{x}_k; W, \\mathbf{x})\n",
    "P(\\mathbf{y}_j|\\Delta \\mathbf{x}_l; W, \\mathbf{x})\n",
    "S(\\mathbf{y}_i, \\mathbf{y}_j)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous case:\n",
    "\n",
    "$\\Delta \\mathbf{x} \\in \\mathbb{R}^N$\n",
    "\n",
    "$\n",
    "\\mathop{{}\\mathbb{E}}[S_{in}] \\equiv\n",
    "\\int d\\Delta\\mathbf{x} P(\\Delta \\mathbf{x}) \n",
    "\\sum_{i,j} \n",
    "P(\\mathbf{y}_i|\\Delta \\mathbf{x}; W, \\mathbf{x})\n",
    "P(\\mathbf{y}_j|\\Delta \\mathbf{x}; W, \\mathbf{x})\n",
    "S(\\mathbf{y}_i, \\mathbf{y}_j)\n",
    "$\n",
    "\n",
    "Define:\n",
    "\n",
    "$T(\\Delta \\mathbf{x}_k, \\Delta \\mathbf{x}_l) \\equiv \\textrm{distance metric between excitability-change vectors}$\n",
    "\n",
    "Then:\n",
    "\n",
    "$\n",
    "\\mathop{{}\\mathbb{E}}[S_{bt}] \\equiv \n",
    "\\int d\\Delta\\mathbf{x}_k d\\Delta\\mathbf{x}_l\n",
    "P(\\Delta \\mathbf{x}_k) P(\\Delta \\mathbf{x}_l)\n",
    "T(\\Delta \\mathbf{x}_k, \\Delta \\mathbf{x}_l)\n",
    "\\sum_{i,j}\n",
    "P(\\mathbf{y}_i|\\Delta \\mathbf{x}_k; W, \\mathbf{x})\n",
    "P(\\mathbf{y}_j|\\Delta \\mathbf{x}_l; W, \\mathbf{x})\n",
    "S(\\mathbf{y}_i, \\mathbf{y}_j)\n",
    "$\n",
    "\n",
    "Here, $T(\\Delta \\mathbf{x}_k, \\Delta \\mathbf{x}_l)$ excludes similar pairs $\\Delta \\mathbf{x}_k, \\Delta \\mathbf{x}_l$ such that $\\mathop{{}\\mathbb{E}}[S_{bt}]$ quantifies the similarity between activity sequences conditioned on sufficiently different $\\Delta \\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What $\\{W, \\mathbf{x}\\}$ maximize $D_x^y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation\n",
    "\n",
    "Again the optimization problem is almost certainly intractable.\n",
    "\n",
    "Additionally, it is not obvious how to estimate $D_x^y$ even given $W$, due to the required sampling over $\\Delta \\mathbf{x}$. In particular, we expect there only to be a small subset of $\\Delta \\mathbf{x}$ that dramatically alter activity sequences, intuitively those in which neurons are excited that line up with \"paths\" through the network. However, it seems unlikely that we'd stumble upon these via random sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define:\n",
    "\n",
    "$S_{in}, S_{bt}$ to be two given similarity values, which we shall use as two cutoff levels for $S(\\mathbf{r}_i(t), \\mathbf{r}_j(t))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an \"excitability control\" $\\Delta \\mathbf{x}^*$:\n",
    "\n",
    "$\\Delta \\mathbf{x}^* \\in \\{\\Delta \\mathbf{x}\\}$ s.t.\n",
    "\n",
    "$\\mathop{{}\\mathbb{E}}[S(\\mathbf{r}_i(t), \\mathbf{r}_j(t))] = \n",
    "\\int d\\mathbf{r}_i(t)d\\mathbf{r}_j(t)\n",
    "P(\\mathbf{r}_i(t)|\\Delta \\mathbf{x}^*)\n",
    "P(\\mathbf{r}_j(t)|\\Delta \\mathbf{x}^*)\n",
    "S(\\mathbf{r}_i(t), \\mathbf{r}_j(t)) \\geq S_{in}$\n",
    "\n",
    "i.e. an excitability control is an excitability vector that generates sufficiently similar network songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a network's \"excitable song capacity\" $C_E(W, \\mathbf{x})$ as the maximum number of excitability controls for a network s.t.\n",
    "\n",
    "$\\forall k\\neq l:\n",
    "\\mathop{{}\\mathbb{E}}[S(\\mathbf{r}_k(t), \\mathbf{r}_l(t))] =\n",
    "\\int d\\mathbf{r}_k(t) d\\mathbf{r}_l(t)\n",
    "P(\\mathbf{r}_k(t)|\\Delta \\mathbf{x}_k^*)\n",
    "P(\\mathbf{r}_l(t)|\\Delta \\mathbf{x}_l^*)\n",
    "S(\\mathbf{r}_k(t), \\mathbf{r}_l(t)) < S_{out}\n",
    "$.\n",
    "\n",
    "This quantifies the number of discriminable sequences that can be embedded into a network through excitability changes alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does $C_E$ depend on $W$ and $\\mathbf{x}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, estimating $C_E$ for all but the simplest $W$ is likely intractable, due to the sampling over $\\Delta \\mathbf{x}$ that would be required (since even for networks with high $C_E$, the subset of $\\Delta \\mathbf{x}$'s that are valid excitability controls would still be a tiny fraction of all $\\Delta \\mathbf{x}$'s).\n",
    "\n",
    "However, we can gain insight into the question by considering the simplified case in which the result of applying $\\Delta \\mathbf{x}$ is to move a subset of neurons to a resting potential near their spiking threshold and to hold the remainder near the inhibitory reversal potential, preventing them from spiking.\n",
    "\n",
    "Since in this case spiking will be constrained to the subset of excitable neurons, we can  ask how the connections among them, themselves a subset of the full network's connections, shape sequential activity dynamics among the excitable neurons.\n",
    "\n",
    "By understanding what connectivity structures among the neurons \"selected\" by a single excitability control enable the generation of consistent sequential activity patterns, we can draw inferences about how to embed multiple such structures in a network so that each corresponding excitability control yields a distinct sequential activity pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct our analysis, we introduce two metrics to quantify a given network's propensity to produce consistent sequential activity involving the majority of its neurons.\n",
    "\n",
    "1. Repetition index ($RI_L$): this quantifies how frequently the most common L-length sequence is repeated within a recording, with \"softness\" added to account for imperfect repetitions. Recordings in which one sequence is frequently repeated will thus have a high repetition index, whereas recordings of essentially random activity will have a low repetition index.\n",
    "\n",
    "2. Participation index ($PI$): this quantifies what fraction of neurons are active within a recording. Thus, although a network with a single continuously firing neuron and the rest silent would have a high repetition index, it would have a low participation index.\n",
    "\n",
    "We provide precise definitions of these terms in a moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next use these metrics to evaluate the sequential activity patterns produced by neural networks with varying connectivity structures.\n",
    "\n",
    "In particular, we consider Erdos-Renyi and scale-free networks with varied connection densities, as well as a network of fixed connection density where we vary the fraction of feed-forward connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repetition index\n",
    "\n",
    "The repetition index quantifies how frequently the most common L-length subsequence is repeated within a recording. It is computed accordingly:\n",
    "\n",
    "1. Convert multi-neuron spike train to discrete sequence vector $\\mathbf{y}$, where the i-th element is the index of the neuron that fired the i-th spike.\n",
    "2. Loop over all L-length subsequences $\\mathbf{y}_L$.\n",
    "3. For each $\\mathbf{y}_L$ calculate the expected value $<S_y>$ of its similarity $S_y(\\mathbf{y}_L^k, \\mathbf{y}_L^l)$ to all other L-length subsequences. ($S_y$ is defined below.)\n",
    "4. Return the max $<S_y>$ over all L-length subsequences, i.e. the average similarity of the most common L-length subsequence to all other L-length subsequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Participation index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network construction and stimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Biological activation-triggered excitability changes support short-term sequence replay in a model place cell network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding analyses suggest conditions under which excitability changes substantially influence network dynamics. First, changes in the excitability of highly central neurons, i.e. neurons with many incoming and outgoing connections, affect network dynamics more than changes in neurons with fewer incoming and outgoing connections. Second, multi-neuron excitability changes engender consistent sequential dynamics in a network when the connectivity among the hyperexcitable neurons resembles a chain-like structure.\n",
    "\n",
    "Here we show that such considerations allow to draw mechanistic insights about a cognitively relevant neural phenomenon: short-term sequence replay within place cell networks, most notably observed in the hippocampus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of replay.\n",
    "\n",
    "But mechanism unknown. Unlikely that network is undergoing massive structural changes. Furthermore, this would not be computationally beneficial.\n",
    "\n",
    "An alternative possibility is excitability changes, in particular, activation-triggered excitability changes. Indeed, recent studies of EC->HPC cxns show activation-triggered K+ channel inactivation, which causes distal segments of dendritic arbor to increase resistance. Functional consequence is that under background input to this part of dendrites, cells that have been recently activated will receive stronger EPSPs on average, moving them closer to threshold, i.e. more excitable than non-recently activated cells.\n",
    "\n",
    "We ask whether such a mechanism, observed empirically, is sufficient to generate short-term sequence replay. Specifically, is the voltage differential between recently activated and non-recently activated cells receiving background inputs to distal dendrites sufficient to confine activity reactivation to recently activated cells."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
